{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,cv2,keras\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras.datasets import mnist\n",
    "\n",
    "\n",
    "#load mnist dataset\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def create_image(mnist_image):\n",
    "    #create 72x72 canvas\n",
    "    canvas = np.zeros((72,72), dtype=np.uint8)\n",
    "\n",
    "    #resize mnist image to random size between 14x14 and 36x36\n",
    "    random_size = np.random.randint(14, 36)\n",
    "    mnist_image = cv2.resize(mnist_image, (random_size, random_size))\n",
    "\n",
    "    #paste mnist image to random location on canvas\n",
    "    x_offset = np.random.randint(0, 72-random_size)\n",
    "    y_offset = np.random.randint(0, 72-random_size)\n",
    "    canvas[y_offset:y_offset+random_size, x_offset:x_offset+random_size] = mnist_image\n",
    "\n",
    "    #get object location in canvas coordinates for R-CNN training\n",
    "    x_min = x_offset / 72\n",
    "    y_min = y_offset / 72\n",
    "    x_max = (x_offset + random_size) / 72\n",
    "    y_max = (y_offset + random_size) / 72\n",
    "\n",
    "    location = [x_min, y_min, x_max, y_max]\n",
    "\n",
    "    return canvas, location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4444444444444444 0.041666666666666664 0.7777777777777778 0.375\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "temp,location = create_image(x_train[0])\n",
    "plt.imshow(temp, cmap='gray')\n",
    "print(*location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60000/60000 [00:00<00:00, 89552.23it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 89285.91it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "temp_x_train = np.ndarray((x_train.shape[0], 72, 72), dtype=np.uint8)\n",
    "locations_train = np.ndarray((x_train.shape[0], 4), dtype=np.float32)\n",
    "\n",
    "for i in tqdm(range(x_train.shape[0])):\n",
    "    temp_x_train[i], label = create_image(x_train[i])\n",
    "    locations_train[i] = label\n",
    "\n",
    "x_train = temp_x_train\n",
    "\n",
    "temp_x_test = np.ndarray((x_test.shape[0], 72, 72), dtype=np.uint8)\n",
    "locations_test = np.ndarray((x_test.shape[0], 4), dtype=np.float32)\n",
    "\n",
    "for i in tqdm(range(x_test.shape[0])):\n",
    "    temp_x_test[i], label = create_image(x_test[i])\n",
    "    locations_test[i] = label\n",
    "\n",
    "x_test = temp_x_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train R-CNN with TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Emre\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 74\u001b[0m\n\u001b[0;32m     71\u001b[0m y_test_onehot \u001b[38;5;241m=\u001b[39m to_categorical(y_test, num_classes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m)\n\u001b[0;32m     73\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m---> 74\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mlocations_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_onehot\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mx_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mlocations_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test_onehot\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     76\u001b[0m \u001b[38;5;66;03m# Evaluate the model\u001b[39;00m\n\u001b[0;32m     77\u001b[0m loss \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mevaluate(x_test, [locations_test, y_test_onehot])\n",
      "File \u001b[1;32mc:\\Users\\Emre\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\Emre\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\engine\\training.py:1564\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1556\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[0;32m   1557\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1558\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1561\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m   1562\u001b[0m ):\n\u001b[0;32m   1563\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1564\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1565\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[0;32m   1566\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[1;32mc:\\Users\\Emre\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\Emre\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    912\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    914\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 915\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    917\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    918\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32mc:\\Users\\Emre\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:980\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    976\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m  \u001b[38;5;66;03m# Fall through to cond-based initialization.\u001b[39;00m\n\u001b[0;32m    977\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    978\u001b[0m     \u001b[38;5;66;03m# Lifting succeeded, so variables are initialized and we can run the\u001b[39;00m\n\u001b[0;32m    979\u001b[0m     \u001b[38;5;66;03m# stateless function.\u001b[39;00m\n\u001b[1;32m--> 980\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateless_fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    981\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    982\u001b[0m   _, _, filtered_flat_args \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    983\u001b[0m       \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateful_fn\u001b[38;5;241m.\u001b[39m_function_spec\u001b[38;5;241m.\u001b[39mcanonicalize_function_inputs(  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m    984\u001b[0m           \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds))\n",
      "File \u001b[1;32mc:\\Users\\Emre\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:2496\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2493\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m   2494\u001b[0m   (graph_function,\n\u001b[0;32m   2495\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2496\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2497\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Emre\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:1862\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1858\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1859\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1860\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1861\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1862\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1863\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcancellation_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcancellation_manager\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m   1864\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1865\u001b[0m     args,\n\u001b[0;32m   1866\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1867\u001b[0m     executing_eagerly)\n\u001b[0;32m   1868\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32mc:\\Users\\Emre\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:499\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    497\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _InterpolateFunctionError(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    498\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m cancellation_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 499\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_num_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    503\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    504\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    505\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    506\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m    507\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[0;32m    508\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    511\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx,\n\u001b[0;32m    512\u001b[0m         cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager)\n",
      "File \u001b[1;32mc:\\Users\\Emre\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 54\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     57\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "\n",
    "def iou(y_true, y_pred):\n",
    "    # Define the intersection and the union\n",
    "    xA = K.maximum(y_true[:, 0], y_pred[:, 0])\n",
    "    yA = K.maximum(y_true[:, 1], y_pred[:, 1])\n",
    "    xB = K.minimum(y_true[:, 2], y_pred[:, 2])\n",
    "    yB = K.minimum(y_true[:, 3], y_pred[:, 3])\n",
    "\n",
    "    # Calculate the area of intersection\n",
    "    interArea = K.maximum(0.0, xB - xA) * K.maximum(0.0, yB - yA)\n",
    "\n",
    "    # Calculate the area of both rectangles\n",
    "    boxAArea = (y_true[:, 2] - y_true[:, 0]) * (y_true[:, 3] - y_true[:, 1])\n",
    "    boxBArea = (y_pred[:, 2] - y_pred[:, 0]) * (y_pred[:, 3] - y_pred[:, 1])\n",
    "\n",
    "    # Calculate the area of union\n",
    "    union = (boxAArea + boxBArea) - interArea\n",
    "\n",
    "    # Calculate the IOU\n",
    "    iou = interArea / union\n",
    "\n",
    "    return iou\n",
    "\n",
    "# Define R-CNN model\n",
    "def RCNN(image):\n",
    "    x = Conv2D(32, (3, 3), activation='relu', input_shape=(72, 72, 1))(image)\n",
    "    x = MaxPooling2D((2, 2))(x)\n",
    "    x = Conv2D(64, (3, 3), activation='relu')(x)\n",
    "    x = MaxPooling2D((2, 2))(x)\n",
    "    x = Conv2D(64, (3, 3), activation='relu')(x)\n",
    "    x_1 = Flatten()(x)\n",
    "    x_1 = Dense(64, activation='relu')(x_1)\n",
    "    x_1 = Dense(4)(x_1)\n",
    "\n",
    "    x_2 = Flatten()(x)\n",
    "    x_2 = Dense(64, activation='relu')(x_2)\n",
    "    x_2 = Dense(10)(x_2)\n",
    "\n",
    "    return x_1, x_2\n",
    "\n",
    "input_shape = (72, 72, 1)\n",
    "image_input = Input(shape=input_shape)\n",
    "output_1, output_2 = RCNN(image_input)\n",
    "\n",
    "model = Model(inputs=image_input, outputs=[output_1, output_2])\n",
    "optimizer = Adam(lr=0.001)\n",
    "\n",
    "# Compile the model with IOU loss function\n",
    "model.compile(optimizer=optimizer, loss=[iou, 'categorical_crossentropy'], metrics=[])\n",
    "\n",
    "\n",
    "# Reshape data for CNN input\n",
    "x_train = np.expand_dims(x_train, axis=-1)\n",
    "x_test = np.expand_dims(x_test, axis=-1)\n",
    "\n",
    "# Normalize pixel values to range [0, 1]\n",
    "x_train = x_train.astype('float32') / 255\n",
    "x_test = x_test.astype('float32') / 255\n",
    "\n",
    "y_train_onehot = to_categorical(y_train, num_classes=10)\n",
    "y_test_onehot = to_categorical(y_test, num_classes=10)\n",
    "\n",
    "# Train the model\n",
    "model.fit(x_train, [locations_train, y_train_onehot], epochs=5, batch_size=32, validation_data=(x_test, [locations_test, y_test_onehot]))\n",
    "\n",
    "# Evaluate the model\n",
    "loss = model.evaluate(x_test, [locations_test, y_test_onehot])\n",
    "print(\"Test Loss:\", loss)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#write a function to predict bounding box coordinates\n",
    "def predict_bounding_box_and_label(image):\n",
    "    #resize image to 72x72\n",
    "    image = cv2.resize(image, (72, 72))\n",
    "\n",
    "    #reshape and normalize pixel values\n",
    "    image = np.expand_dims(image, axis=-1)\n",
    "    image = np.expand_dims(image, axis=0)\n",
    "    image = image.astype('float32') / 255\n",
    "\n",
    "    #predict bounding box coordinates\n",
    "    bounding_box, label  = model.predict(image)[0]\n",
    "\n",
    "    return bounding_box, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 15ms/step\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 2, got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[42], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m test_image \u001b[38;5;241m=\u001b[39m x_test[\u001b[38;5;241m6\u001b[39m]\n\u001b[1;32m----> 2\u001b[0m bounding_box, label \u001b[38;5;241m=\u001b[39m predict_bounding_box(test_image)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m#draw bounding box on test image\u001b[39;00m\n\u001b[0;32m      5\u001b[0m image \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msqueeze(test_image)\n",
      "\u001b[1;31mValueError\u001b[0m: not enough values to unpack (expected 2, got 1)"
     ]
    }
   ],
   "source": [
    "test_image = x_test[6]\n",
    "bounding_box , label = predict_bounding_box(test_image)\n",
    "\n",
    "#draw bounding box on test image\n",
    "image = np.squeeze(test_image)\n",
    "image = cv2.resize(image, (72, 72))\n",
    "image = cv2.cvtColor(image, cv2.COLOR_GRAY2BGR)\n",
    "\n",
    "#convert normalized coordinates to pixel values\n",
    "x_min = int(bounding_box[0] * 72)\n",
    "y_min = int(bounding_box[1] * 72)\n",
    "x_max = int(bounding_box[2] * 72)\n",
    "y_max = int(bounding_box[3] * 72)\n",
    "\n",
    "#draw bounding box\n",
    "image = cv2.rectangle(image, (x_min, y_min), (x_max, y_max), (0, 255, 0), 1)\n",
    "\n",
    "plt.imshow(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60000/60000 [00:00<00:00, 87412.11it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 89552.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 72, 72, 1)]  0           []                               \n",
      "                                                                                                  \n",
      " conv2d (Conv2D)                (None, 70, 70, 32)   320         ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " max_pooling2d (MaxPooling2D)   (None, 35, 35, 32)   0           ['conv2d[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_1 (Conv2D)              (None, 33, 33, 64)   18496       ['max_pooling2d[0][0]']          \n",
      "                                                                                                  \n",
      " max_pooling2d_1 (MaxPooling2D)  (None, 16, 16, 64)  0           ['conv2d_1[0][0]']               \n",
      "                                                                                                  \n",
      " conv2d_2 (Conv2D)              (None, 14, 14, 64)   36928       ['max_pooling2d_1[0][0]']        \n",
      "                                                                                                  \n",
      " flatten (Flatten)              (None, 12544)        0           ['conv2d_2[0][0]']               \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 64)           802880      ['flatten[0][0]']                \n",
      "                                                                                                  \n",
      " bounding_box (Dense)           (None, 4)            260         ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " class_label (Dense)            (None, 10)           650         ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 859,534\n",
      "Trainable params: 859,534\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Emre\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "Epoch 1/5\n",
      "1875/1875 [==============================] - 13s 5ms/step - loss: 1.6302 - bounding_box_loss: 0.8518 - class_label_loss: 0.7783 - bounding_box_accuracy: 0.6105 - class_label_accuracy: 0.7329 - val_loss: 1.0823 - val_bounding_box_loss: 0.7171 - val_class_label_loss: 0.3652 - val_bounding_box_accuracy: 0.8700 - val_class_label_accuracy: 0.8881\n",
      "Epoch 2/5\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.8572 - bounding_box_loss: 0.6004 - class_label_loss: 0.2568 - bounding_box_accuracy: 0.8567 - class_label_accuracy: 0.9277 - val_loss: 0.7139 - val_bounding_box_loss: 0.5111 - val_class_label_loss: 0.2029 - val_bounding_box_accuracy: 0.9125 - val_class_label_accuracy: 0.9417\n",
      "Epoch 3/5\n",
      "1875/1875 [==============================] - 9s 5ms/step - loss: 0.6007 - bounding_box_loss: 0.4401 - class_label_loss: 0.1606 - bounding_box_accuracy: 0.9205 - class_label_accuracy: 0.9544 - val_loss: 0.5423 - val_bounding_box_loss: 0.4159 - val_class_label_loss: 0.1264 - val_bounding_box_accuracy: 0.9306 - val_class_label_accuracy: 0.9611\n",
      "Epoch 4/5\n",
      "1875/1875 [==============================] - 9s 5ms/step - loss: 0.4870 - bounding_box_loss: 0.3754 - class_label_loss: 0.1117 - bounding_box_accuracy: 0.9391 - class_label_accuracy: 0.9684 - val_loss: 0.4874 - val_bounding_box_loss: 0.3617 - val_class_label_loss: 0.1257 - val_bounding_box_accuracy: 0.9282 - val_class_label_accuracy: 0.9618\n",
      "Epoch 5/5\n",
      "1875/1875 [==============================] - 9s 5ms/step - loss: 0.4206 - bounding_box_loss: 0.3396 - class_label_loss: 0.0810 - bounding_box_accuracy: 0.9481 - class_label_accuracy: 0.9768 - val_loss: 0.4256 - val_bounding_box_loss: 0.3212 - val_class_label_loss: 0.1045 - val_bounding_box_accuracy: 0.9494 - val_class_label_accuracy: 0.9668\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1fb46605ac0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os,cv2,keras\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras.datasets import mnist\n",
    "from tqdm import tqdm\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# load mnist dataset\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "y_train_encoded = to_categorical(y_train, num_classes=10)\n",
    "y_test_encoded = to_categorical(y_test, num_classes=10)\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def create_image(mnist_image):\n",
    "    # create 72x72 canvas\n",
    "    canvas = np.zeros((72,72), dtype=np.uint8)\n",
    "\n",
    "    # resize mnist image to random size between 14x14 and 36x36\n",
    "    random_size = np.random.randint(14, 36)\n",
    "    mnist_image = cv2.resize(mnist_image, (random_size, random_size))\n",
    "\n",
    "    # paste mnist image to random location on canvas\n",
    "    x_offset = np.random.randint(0, 72-random_size)\n",
    "    y_offset = np.random.randint(0, 72-random_size)\n",
    "    canvas[y_offset:y_offset+random_size, x_offset:x_offset+random_size] = mnist_image\n",
    "\n",
    "    # get object location in canvas coordinates for R-CNN training\n",
    "    x_min = x_offset / 72\n",
    "    y_min = y_offset / 72\n",
    "    x_max = (x_offset + random_size) / 72\n",
    "    y_max = (y_offset + random_size) / 72\n",
    "\n",
    "    location = [x_min, y_min, x_max, y_max]\n",
    "\n",
    "    return canvas, location\n",
    "\n",
    "temp_x_train = np.ndarray((x_train.shape[0], 72, 72), dtype=np.uint8)\n",
    "locations_train = np.ndarray((x_train.shape[0], 4), dtype=np.float32)\n",
    "\n",
    "for i in tqdm(range(x_train.shape[0])):\n",
    "    temp_x_train[i], label = create_image(x_train[i])\n",
    "    locations_train[i] = label\n",
    "\n",
    "x_train = temp_x_train\n",
    "\n",
    "temp_x_test = np.ndarray((x_test.shape[0], 72, 72), dtype=np.uint8)\n",
    "locations_test = np.ndarray((x_test.shape[0], 4), dtype=np.float32)\n",
    "\n",
    "for i in tqdm(range(x_test.shape[0])):\n",
    "    temp_x_test[i], label = create_image(x_test[i])\n",
    "    locations_test[i] = label\n",
    "\n",
    "x_test = temp_x_test\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "def iou(y_true, y_pred):\n",
    "    # Define the intersection and the union\n",
    "    xA = K.maximum(y_true[0][0], y_pred[0][0]) \n",
    "    yA = K.maximum(y_true[0][1], y_pred[0][1])\n",
    "    xB = K.minimum(y_true[0][2], y_pred[0][2])\n",
    "    yB = K.minimum(y_true[0][3], y_pred[0][3])\n",
    "\n",
    "    # Calculate the area of intersection\n",
    "    interArea = K.maximum(0.0, xB - xA) * K.maximum(0.0, yB - yA)\n",
    "\n",
    "    # Calculate the area of both rectangles\n",
    "    boxAArea = (y_true[0][2] - y_true[0][0]) * (y_true[0][3] - y_true[0][1])\n",
    "    boxBArea = (y_pred[0][2] - y_pred[0][0]) * (y_pred[0][3] - y_pred[0][1])\n",
    "\n",
    "    # Calculate the area of union\n",
    "    union = (boxAArea + boxBArea) - interArea\n",
    "\n",
    "    # Calculate the IOU\n",
    "    iou = interArea / union\n",
    "\n",
    "    return 1-iou\n",
    "\n",
    "# Define R-CNN model\n",
    "def RCNN(image):\n",
    "    x = Conv2D(32, (3, 3), activation='relu', input_shape=(72, 72, 1))(image)\n",
    "    x = MaxPooling2D((2, 2))(x)\n",
    "    x = Conv2D(64, (3, 3), activation='relu')(x)\n",
    "    x = MaxPooling2D((2, 2))(x)\n",
    "    x = Conv2D(64, (3, 3), activation='relu')(x)\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(64, activation='relu')(x)\n",
    "    \n",
    "    # Output for bounding box prediction\n",
    "    bounding_box_output = Dense(4, activation= 'sigmoid', name = 'bounding_box')(x)\n",
    "\n",
    "    # Output for label prediction\n",
    "    label_output = Dense(10, activation='softmax', name = 'class_label')(x)\n",
    "\n",
    "    return bounding_box_output, label_output\n",
    "\n",
    "input_shape = (72, 72, 1)\n",
    "image_input = Input(shape=input_shape)\n",
    "output_1, output_2 = RCNN(image_input)\n",
    "\n",
    "model = Model(inputs=image_input, outputs=[output_1, output_2])\n",
    "optimizer = Adam(lr=0.001)\n",
    "\n",
    "losses = {\n",
    "    \"class_label\": \"categorical_crossentropy\",\n",
    "    \"bounding_box\": iou\n",
    "}\n",
    "\n",
    "lossWeights = {\n",
    "    \"class_label\": 1.0,\n",
    "    \"bounding_box\": 1.0\n",
    "}\n",
    "\n",
    "trainTargets = {\n",
    "    \"class_label\": y_train_encoded,\n",
    "    \"bounding_box\": locations_train\n",
    "}\n",
    "\n",
    "testTargets = {\n",
    "    \"class_label\": y_test_encoded,\n",
    "    \"bounding_box\": locations_test\n",
    "}\n",
    "\n",
    "# Compile the model with IOU loss function\n",
    "model.compile(optimizer=optimizer, loss=losses, metrics=['accuracy'], loss_weights=lossWeights)\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "# Reshape data for CNN input\n",
    "x_train = np.expand_dims(x_train, axis=-1)\n",
    "x_test = np.expand_dims(x_test, axis=-1)\n",
    "\n",
    "# Normalize pixel values to range [0, 1]\n",
    "x_train = x_train.astype('float32') / 255\n",
    "x_test = x_test.astype('float32') / 255\n",
    "\n",
    "# Train the model\n",
    "model.fit(x_train, trainTargets, epochs=5, batch_size=32, validation_data=(x_test, testTargets))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 1s 3ms/step - loss: 0.4256 - bounding_box_loss: 0.3212 - class_label_loss: 0.1045 - bounding_box_accuracy: 0.9494 - class_label_accuracy: 0.9668\n",
      "Test Loss: [0.42563074827194214, 0.3211584985256195, 0.10447245091199875, 0.949400007724762, 0.9667999744415283]\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "loss = model.evaluate(x_test, testTargets)\n",
    "print(\"Test Loss:\", loss)\n",
    "\n",
    "# Write a function to predict bounding box coordinates\n",
    "def predict_bounding_box_and_label(image):\n",
    "    # Resize image to 72x72\n",
    "    image = cv2.resize(image, (72, 72))\n",
    "\n",
    "    # Reshape and normalize pixel values\n",
    "    image = np.expand_dims(image, axis=-1)\n",
    "    image = np.expand_dims(image, axis=0)\n",
    "    image = image.astype('float32') / 255\n",
    "\n",
    "    # Predict bounding box coordinates\n",
    "    bounding_box, label = model.predict(image)\n",
    "    \n",
    "    return bounding_box, label\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(i):\n",
    "    test_image = x_test[i]\n",
    "    bounding_box , label = predict_bounding_box_and_label(test_image)\n",
    "\n",
    "    # draw bounding box on test image\n",
    "    image = np.squeeze(test_image)\n",
    "    image = cv2.resize(image, (72, 72))\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_GRAY2BGR)\n",
    "\n",
    "    # convert normalized coordinates to pixel values\n",
    "    x_min = int(bounding_box[0][0] * 72)\n",
    "    y_min = int(bounding_box[0][1] * 72)\n",
    "    x_max = int(bounding_box[0][2] * 72)\n",
    "    y_max = int(bounding_box[0][3] * 72)\n",
    "\n",
    "    print(\"Predicted Label:\", np.argmax(label))\n",
    "    print(\"Bounding Box Coordinates:\", bounding_box)\n",
    "    #print actual label\n",
    "    print(\"Actual Label:\", y_test[i])\n",
    "    print(\"Actual Bounding Box Coordinates:\", locations_test[i])\n",
    "\n",
    "\n",
    "    # draw bounding box\n",
    "    # Adjusting coordinates to match the format expected by cv2.rectangle\n",
    "    top_left = (x_min, y_min)\n",
    "    bottom_right = (x_max, y_max)\n",
    "    image = cv2.rectangle(image, top_left, bottom_right, (0, 255, 0), 1)\n",
    "\n",
    "    plt.imshow(image)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 18ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Label: 1\n",
      "Bounding Box Coordinates: [[0.41371313 0.31117204 0.65524626 0.58643466]]\n",
      "Actual Label: 4\n",
      "Actual Bounding Box Coordinates: [0.33333334 0.30555555 0.7916667  0.7638889 ]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
